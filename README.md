
# Health Data Pipeline & Analytics Platform

> **Desafio T√©cnico** > Solu√ß√£o Full Stack para Engenharia de Dados na Sa√∫de Suplementar.

Este reposit√≥rio cont√©m uma solu√ß√£o completa para o ciclo de vida de dados da ANS (Ag√™ncia Nacional de Sa√∫de Suplementar), abrangendo desde a extra√ß√£o automatizada (ETL) at√© a visualiza√ß√£o em dashboard web.

---

## üóÇÔ∏è Estrutura Modular do Projeto

O projeto foi arquitetado em 4 m√≥dulos independentes, facilitando a manuten√ß√£o, escalabilidade e separa√ß√£o de responsabilidades:

```text
/
‚îú‚îÄ‚îÄ 01_etl_ans/           # [Java 21] Pipeline de Extra√ß√£o, Transforma√ß√£o e Carga (ETL)
‚îú‚îÄ‚îÄ 02_validacao_dados/   # [Python] Scripts de valida√ß√£o de CNPJ e enriquecimento
‚îú‚îÄ‚îÄ 03_analise_sql/       # [SQL] Modelagem Dimensional e Queries Anal√≠ticas
‚îú‚îÄ‚îÄ 04_plataforma_web/    # [Vue.js + Python] Dashboard e API REST
‚îî‚îÄ‚îÄ data/                 # Data Lake local (Raw e Processed)

```

---

## üöÄ M√≥dulo 1: ETL de Demonstra√ß√µes Cont√°beis (Java)

**Localiza√ß√£o:** [`./01_etl_ans`](https://www.google.com/search?q=./01_etl_ans)

**Tecnologia:** Java 21 (LTS), Maven, JUnit 5.

Este m√≥dulo √© o cora√ß√£o da ingest√£o de dados. Ele automatiza a coleta de demonstra√ß√µes cont√°beis trimestrais, resolve inconsist√™ncias de formato e consolida os dados para an√°lise posterior.

### üõ†Ô∏è Arquitetura da Solu√ß√£o

O pipeline executa 4 est√°gios sequenciais e at√¥micos:

1. **Crawler Inteligente (`CrawlerANS`):**
* Varre recursivamente o servidor FTP da ANS.
* Identifica dinamicamente os 3 trimestres mais recentes dispon√≠veis (via Regex).
* Realiza o download resiliente, detectando automaticamente se o alvo √© um arquivo `.zip` direto ou uma estrutura de diret√≥rios.


2. **Descompacta√ß√£o Segura (`Descompactador`):**
* Extrai os arquivos CSV para uma √°rea de *staging* tempor√°ria.
* For√ßa o encoding **ISO-8859-1** para garantir a leitura correta de caracteres acentuados (padr√£o legado governamental).


3. **Parsing & Normaliza√ß√£o (`LeitorCSV`):**
* Detecta automaticamente varia√ß√µes de schema (ex: colunas `VL_SALDO_FINAL` vs `Valor`).
* Converte formata√ß√£o monet√°ria brasileira (PT-BR) para `BigDecimal`.
* Enriquece os dados extraindo Ano e Trimestre diretamente do nome do arquivo (Fonte de verdade).


4. **Consolida√ß√£o (`Consolidador`):**
* Aplica regras de neg√≥cio (filtragem de dados inconsistentes).
* Gera o arquivo final unificado `consolidado_despesas.csv` e o compacta em ZIP.



---

## üß† Decis√µes T√©cnicas e Trade-offs (An√°lise Cr√≠tica)

Conforme os crit√©rios de avalia√ß√£o do desafio, abaixo est√£o as justificativas para as decis√µes de engenharia adotadas:

### 1. Estrat√©gia de Processamento: *In-Memory* vs *Streaming*

* **Decis√£o:** Processamento em Mem√≥ria (Listas).
* **Contexto:** O volume de dados dos √∫ltimos 3 trimestres (~2.1 milh√µes de registros) ocupa aproximadamente 600MB na Heap da JVM.
* **Justificativa:** Optou-se pela abordagem *In-Memory* para reduzir a complexidade acidental do c√≥digo (*KISS - Keep It Simple*) e permitir opera√ß√µes r√°pidas sem o overhead de I/O constante.
* **Performance:** O tempo total de execu√ß√£o (~11 segundos) valida que a mem√≥ria n√£o √© um gargalo para este volume de dados. Caso o requisito mudasse para "Hist√≥rico de 10 anos", a arquitetura seria refatorada para *Streaming*.

### 2. Tratamento de Inconsist√™ncias (Qualidade de Dados)

* **Remo√ß√£o de Valores <= 0:** Em an√°lises de despesas assistenciais para BI, valores negativos (geralmente estornos cont√°beis) distorcem as m√©tricas de agrega√ß√£o. A limpeza removeu cerca de 40% de "ru√≠do" do dataset, aumentando a precis√£o anal√≠tica.
* **Datas via Metadados:** As colunas de data dentro dos arquivos CSV originais apresentaram instabilidade de formato. A extra√ß√£o do per√≠odo via Regex no nome do arquivo (ex: `1T2025.csv`) garantiu consist√™ncia temporal absoluta.

### 3. Stack Tecnol√≥gica (Java 21)

* A escolha do **Java 21** permitiu o uso de *Text Blocks* e m√©todos modernos de Cole√ß√µes (`List.getFirst()`), resultando em um c√≥digo mais limpo, leg√≠vel e seguro comparado a vers√µes legadas (Java 8/11).

---

## ‚úÖ Diferenciais Implementados

Este projeto implementa requisitos de **Qualidade de Software** listados como diferenciais no descritivo da vaga:

* **üß™ Testes Automatizados:**
* Implementa√ß√£o de testes unit√°rios com **JUnit 5** e **AssertJ**.
* Cobertura cr√≠tica da classe `LeitorCSV`, validando: convers√£o monet√°ria, encoding ISO-8859-1 e extra√ß√£o de datas.


* **‚ö° Performance e Resili√™ncia:**
* Uso de `User-Agent` rotativo no Crawler para evitar bloqueios (HTTP 403).
* Parsing otimizado com `Apache Commons CSV`.


* **üèóÔ∏è Organiza√ß√£o e Boas Pr√°ticas:**
* Separa√ß√£o clara de responsabilidades (SRP) em pacotes (`etl`, `model`, `parser`).
* Logs profissionais (`java.util.logging`) para rastreabilidade de execu√ß√£o.



---

## ‚ñ∂Ô∏è Como Executar

### Pr√©-requisitos

* Java JDK 21+
* Maven 3.8+

### Passo a Passo

1. Acesse o diret√≥rio do m√≥dulo:
```bash
cd 01_etl_ans

```


2. **Execute os testes unit√°rios** (Para validar a integridade do c√≥digo):
```bash
mvn test

```


3. **Execute o Pipeline ETL** (Para gerar os dados):
```bash
mvn clean install
java -cp target/classes:target/dependency/* br.com.seu.Main

```



**Resultado:** O arquivo processado final estar√° dispon√≠vel em:
`data/processed/consolidado_despesas.zip`

---

**Autor:** Nicolas Harnisch

