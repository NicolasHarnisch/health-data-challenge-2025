# Health Data Pipeline & Analytics Platform

> **Desafio T√©cnico** > Solu√ß√£o Full Stack para Engenharia de Dados na Sa√∫de Suplementar.

Este reposit√≥rio cont√©m uma solu√ß√£o completa para o ciclo de vida de dados da ANS (Ag√™ncia Nacional de Sa√∫de Suplementar), abrangendo desde a extra√ß√£o automatizada (ETL) at√© a visualiza√ß√£o em dashboard web.

---

## üóÇÔ∏è Estrutura Modular do Projeto

O projeto foi arquitetado em 4 m√≥dulos independentes, facilitando a manuten√ß√£o, escalabilidade e separa√ß√£o de responsabilidades:

```text
/
‚îú‚îÄ‚îÄ 01_etl_ans/           # [Java 21] Pipeline de Extra√ß√£o, Transforma√ß√£o e Carga (ETL)
‚îú‚îÄ‚îÄ 02_validacao_dados/   # [Java 21] Valida√ß√£o de CNPJ e Enriquecimento Estat√≠stico
‚îú‚îÄ‚îÄ 03_analise_sql/       # [SQL] Modelagem Dimensional e Queries Anal√≠ticas
‚îú‚îÄ‚îÄ 04_plataforma_web/    # [Python + Vue.js] API REST (FastAPI) e Dashboard
‚îî‚îÄ‚îÄ data/                 # Data Lake local (Raw e Processed)

```

---

## üöÄ M√≥dulo 1: ETL de Demonstra√ß√µes Cont√°beis (Java)

**Localiza√ß√£o:** [`./01_etl_ans`](https://www.google.com/search?q=./01_etl_ans)

**Tecnologia:** Java 21 (LTS), Maven, JUnit 5.

Este m√≥dulo √© o cora√ß√£o da ingest√£o de dados. Ele automatiza a coleta de demonstra√ß√µes cont√°beis trimestrais, resolve inconsist√™ncias de formato e consolida os dados para an√°lise posterior.

### üõ†Ô∏è Arquitetura da Solu√ß√£o

O pipeline executa 4 est√°gios sequenciais e at√¥micos:

1. **Crawler Inteligente (`CrawlerANS`):**
* Varre recursivamente o servidor FTP da ANS.
* Identifica dinamicamente os 3 trimestres mais recentes dispon√≠veis (via Regex).
* Realiza o download resiliente, detectando automaticamente se o alvo √© um arquivo `.zip` direto ou uma estrutura de diret√≥rios.


2. **Descompacta√ß√£o Segura (`Descompactador`):**
* Extrai os arquivos CSV para uma √°rea de *staging* tempor√°ria.
* For√ßa o encoding **ISO-8859-1** para garantir a leitura correta de caracteres acentuados (padr√£o legado governamental).


3. **Parsing & Normaliza√ß√£o (`LeitorCSV`):**
* Detecta automaticamente varia√ß√µes de schema (ex: colunas `VL_SALDO_FINAL` vs `Valor`).
* Converte formata√ß√£o monet√°ria brasileira (PT-BR) para `BigDecimal`.
* Enriquece os dados extraindo Ano e Trimestre diretamente do nome do arquivo (Fonte de verdade).


4. **Consolida√ß√£o (`Consolidador`):**
* Aplica regras de neg√≥cio (filtragem de dados inconsistentes).
* Gera o arquivo final unificado `consolidado_despesas.csv` e o compacta em ZIP.



---

## üß† Decis√µes T√©cnicas e Trade-offs (An√°lise Cr√≠tica)

Conforme os crit√©rios de avalia√ß√£o do desafio, abaixo est√£o as justificativas para as decis√µes de engenharia adotadas:

### 1. Estrat√©gia de Processamento: *In-Memory* vs *Streaming*

* **Decis√£o:** Processamento em Mem√≥ria (Listas).
* **Contexto:** O volume de dados dos √∫ltimos 3 trimestres (~2.1 milh√µes de registros) ocupa aproximadamente 600MB na Heap da JVM.
* **Justificativa:** Optou-se pela abordagem *In-Memory* para reduzir a complexidade acidental do c√≥digo (*KISS - Keep It Simple*) e permitir opera√ß√µes r√°pidas sem o overhead de I/O constante.
* **Performance:** O tempo total de execu√ß√£o (~11 segundos) valida que a mem√≥ria n√£o √© um gargalo para este volume de dados. Caso o requisito mudasse para "Hist√≥rico de 10 anos", a arquitetura seria refatorada para *Streaming*.

### 2. Tratamento de Inconsist√™ncias (Qualidade de Dados)

* **Remo√ß√£o de Valores <= 0:** Em an√°lises de despesas assistenciais para BI, valores negativos (geralmente estornos cont√°beis) distorcem as m√©tricas de agrega√ß√£o. A limpeza removeu cerca de 40% de "ru√≠do" do dataset, aumentando a precis√£o anal√≠tica.
* **Datas via Metadados:** As colunas de data dentro dos arquivos CSV originais apresentaram instabilidade de formato. A extra√ß√£o do per√≠odo via Regex no nome do arquivo (ex: `1T2025.csv`) garantiu consist√™ncia temporal absoluta.

### 3. Stack Tecnol√≥gica (Java 21)

* A escolha do **Java 21** permitiu o uso de *Text Blocks* e m√©todos modernos de Cole√ß√µes (`List.getFirst()`), resultando em um c√≥digo mais limpo, leg√≠vel e seguro comparado a vers√µes legadas (Java 8/11).

---

## ‚úÖ Diferenciais Implementados

Este projeto implementa requisitos de **Qualidade de Software** listados como diferenciais no descritivo da vaga:

* **üß™ Testes Automatizados:**
* Implementa√ß√£o de testes unit√°rios com **JUnit 5** e **AssertJ**.
* Cobertura cr√≠tica da classe `LeitorCSV`, validando: convers√£o monet√°ria, encoding ISO-8859-1 e extra√ß√£o de datas.


* **‚ö° Performance e Resili√™ncia:**
* Uso de `User-Agent` rotativo no Crawler para evitar bloqueios (HTTP 403).
* Parsing otimizado com `Apache Commons CSV`.


* **üèóÔ∏è Organiza√ß√£o e Boas Pr√°ticas:**
* Separa√ß√£o clara de responsabilidades (SRP) em pacotes (`etl`, `model`, `parser`).
* Logs profissionais (`java.util.logging`) para rastreabilidade de execu√ß√£o.



---

## ‚ñ∂Ô∏è Como Executar

### Pr√©-requisitos

* Java JDK 21+
* Maven 3.8+

### Passo a Passo

1. Acesse o diret√≥rio do m√≥dulo:
```bash
cd 01_etl_ans
```


2. **Execute os testes unit√°rios** (Para validar a integridade do c√≥digo):
```bash
mvn test
```


3. **Execute o Pipeline ETL** (Para gerar os dados):
```bash
mvn clean install
java -cp target/classes:target/dependency/* br.com.seu.Main
```



**Resultado:** O arquivo processado final estar√° dispon√≠vel em:
`data/processed/consolidado_despesas.zip`

---

## üß™ M√≥dulo 2: Valida√ß√£o e Enriquecimento de Dados (Java)

**Localiza√ß√£o:** [`./02_validacao_dados`](./02_validacao_dados)

**Tecnologia:** Java 21 (LTS), Apache Commons CSV, JUnit 5.

Este m√≥dulo atua como a camada de *Quality Assurance* (QA) e Enriquecimento. Ele consome os dados brutos gerados pelo ETL, aplica valida√ß√µes matem√°ticas (CNPJ), cruza com bases externas da ANS e gera m√©tricas estat√≠sticas para suporte √† decis√£o.

### üõ†Ô∏è Arquitetura da Solu√ß√£o

O pipeline executa 4 est√°gios sequenciais e at√¥micos:

1. **Coleta de Refer√™ncia (`BaixadorCadastro`):**
    * Conecta-se √† API de Dados Abertos da ANS para baixar o cadastro atualizado de operadoras.
    * Implementa um cliente HTTP resiliente (simulando *Browser User-Agent*) para evitar bloqueios de seguran√ßa (Erro 403).

2. **Parsing Resiliente (`CsvUtil`):**
    * L√™ arquivos CSV ignorando BOM (*Byte Order Mark*) e varia√ß√µes de encoding (UTF-8 vs ISO-8859-1).
    * Aplica sanitiza√ß√£o de dados: remo√ß√£o de caracteres n√£o num√©ricos e normaliza√ß√£o de nomes.

3. **Valida√ß√£o Matem√°tica (`ValidadorCNPJ`):**
    * Implementa o algoritmo **M√≥dulo 11** para verificar a autenticidade dos d√≠gitos verificadores dos CNPJs.
    * Classifica os registros sem descart√°-los (estrat√©gia de *Soft Validation*).

4. **Enriquecimento & Analytics (`ProcessadorJoin`):**
    * Realiza o cruzamento de dados (*Join*) entre as Despesas Financeiras e o Cadastro da ANS.
    * Calcula m√©tricas agregadas por operadora: Soma Total, M√©dia Trimestral e **Desvio Padr√£o Amostral**.

---

## üß† Decis√µes T√©cnicas e Trade-offs (An√°lise Cr√≠tica)

Conforme os crit√©rios de avalia√ß√£o, abaixo est√£o as justificativas para as decis√µes de engenharia adotadas neste m√≥dulo:

### 1. Estrat√©gia de Join: *Hash Map* vs *Nested Loop*

* **Decis√£o:** *In-Memory Hash Join*.
* **Contexto:** O cadastro de operadoras possui apenas ~1.200 registros, cabendo confortavelmente na mem√≥ria.
* **Justificativa:** Carregar o cadastro em um `HashMap<String, Operadora>` permite acesso com complexidade **O(1)**. Isso torna o cruzamento com as milhares de linhas de despesas exponencialmente mais r√°pido do que uma busca linear ou la√ßos aninhados (O(N*M)).

### 2. Valida√ß√£o de Dados: *Flagging* vs *Dropping*

* **Decis√£o:** *Flagging* (Marcar com `CNPJ_Valido = false`).
* **Contexto:** Registros financeiros cont√™m valores monet√°rios que comp√µem o balan√ßo total.
* **Justificativa:** Em sistemas financeiros, descartar uma linha devido a um erro de digita√ß√£o no cadastro (typo) altera o montante final ("furo no caixa"). A estrat√©gia de marcar o registro permite auditoria posterior sem perda de integridade cont√°bil.

### 3. Resolu√ß√£o de Chaves (An√°lise de Qualidade)

* **Problema:** O uso inicial do **CNPJ** como chave de liga√ß√£o resultou em 100% de falha (0 matches) devido a inconsist√™ncias de formata√ß√£o na fonte.
* **Solu√ß√£o:** Altera√ß√£o da chave prim√°ria de cruzamento para o **N√∫mero de Registro na ANS**.
* **Resultado:** A taxa de sucesso subiu para **~99%**, restando apenas operadoras inativas ou canceladas, que foram tratadas como "NAO ENCONTRADA" para manter a rastreabilidade.

---

## ‚úÖ Diferenciais Implementados

* **üß™ Testes Unit√°rios Matem√°ticos:**
    * Cobertura de testes na classe `ValidadorCNPJ` garantindo a precis√£o do algoritmo M√≥dulo 11.
    * Valida√ß√£o de casos de borda (CNPJs com d√≠gitos iguais, nulos ou formato incorreto).

* **üìä Estat√≠stica Descritiva:**
    * Implementa√ß√£o manual do c√°lculo de **Desvio Padr√£o** (`EstatisticaService`) para identificar volatilidade nas despesas, sem depend√™ncia de bibliotecas pesadas de Data Science.

---

## ‚ñ∂Ô∏è Como Executar

### Passo a Passo

1. Acesse o diret√≥rio do m√≥dulo:
```bash
cd 02_validacao_dados
```

2. **Execute os testes unit√°rios** (Para validar a matem√°tica do CNPJ):
```bash
mvn test
```


3. **Execute o Processamento**:
```bash
mvn clean install
java -cp target/classes:target/dependency/* br.com.seu.Main
```

**Resultado:** O arquivo enriquecido final estar√° dispon√≠vel em:
`data/processed/despesas_agregadas.csv`

---

## üóÑÔ∏è M√≥dulo 3: Banco de Dados e An√°lise SQL

**Localiza√ß√£o:** [`./03_analise_sql/sql`](https://www.google.com/search?q=./03_analise_sql/sql)

**Tecnologia:** MySQL 8.0 (Compat√≠vel com PostgreSQL), SQL ANSI/ISO.

Este m√≥dulo atua como a camada de *Data Warehousing* e Intelig√™ncia. Ele estrutura o esquema do banco de dados, orquestra a carga massiva dos dados processados (ETL) e executa queries anal√≠ticas complexas para extra√ß√£o de insights de neg√≥cio.

### üõ†Ô∏è Arquitetura da Solu√ß√£o

O pipeline de dados no banco segue 3 est√°gios l√≥gicos:

1. **Modelagem Relacional (DDL):**
* Cria√ß√£o de tabelas normalizadas (`operadoras` vs `demonstracoes_financeiras`) com chaves estrangeiras e √≠ndices otimizados para leitura temporal.
* Defini√ß√£o rigorosa de tipagem (`DECIMAL` para valores monet√°rios, `DATE` para refer√™ncias temporais).


2. **Ingest√£o de Alta Performance (ETL):**
* Utiliza√ß√£o do comando `LOAD DATA LOCAL INFILE` para *Bulk Insert*, garantindo carga de milh√µes de registros em segundos.
* **Sanitiza√ß√£o *On-the-Fly*:** Scripts SQL que tratam formata√ß√£o de moeda (`R$`) e datas (`DD/MM/YYYY` -> ISO) durante o processo de carga, sem necessidade de pr√©-processamento externo.


3. **An√°lise de Neg√≥cio (DQL):**
* Execu√ß√£o de queries anal√≠ticas utilizando recursos avan√ßados do SQL moderno, como *Common Table Expressions* (CTEs) e *Window Functions*.



---

## üß† Decis√µes T√©cnicas e Trade-offs (An√°lise Cr√≠tica)

Conforme os requisitos do desafio, abaixo est√£o as justificativas para as decis√µes de modelagem e consulta adotadas:

### 1. Modelagem: Normaliza√ß√£o (Op√ß√£o B) vs Desnormaliza√ß√£o

* **Decis√£o:** Tabelas Relacionais Normalizadas.
* **Contexto:** O dataset cont√©m dados cadastrais repetitivos associados a milh√µes de transa√ß√µes financeiras.
* **Justificativa:** Separar dados cadastrais (Raz√£o Social, UF) das transa√ß√µes economiza gigabytes de armazenamento e garante integridade referencial. Altera√ß√µes cadastrais s√£o refletidas instantaneamente sem a necessidade de *updates* massivos na tabela de fatos, facilitando a manuten√ß√£o.

### 2. Tipos de Dados: `DECIMAL` vs `FLOAT`

* **Decis√£o:** Uso de `DECIMAL(18,2)`.
* **Contexto:** Armazenamento de valores cont√°beis de demonstra√ß√µes financeiras.
* **Justificativa:** Tipos de ponto flutuante (`FLOAT`, `DOUBLE`) introduzem erros de arredondamento em opera√ß√µes de soma massiva. O tipo `DECIMAL` garante precis√£o exata dos centavos, requisito obrigat√≥rio para sistemas financeiros.

### 3. Estrat√©gia de Query: *Window Functions* vs *Subqueries*

* **Decis√£o:** Uso de `AVG() OVER(PARTITION BY...)`.
* **Contexto:** C√°lculo de operadoras consistentemente acima da m√©dia do mercado (Query 3).
* **Justificativa:** Subqueries correlacionadas possuem complexidade quadr√°tica O(N¬≤), tornando a consulta lenta conforme o banco cresce. *Window Functions* permitem calcular a m√©dia do mercado e comparar com a operadora em uma √∫nica passada de leitura (*Table Scan*), garantindo escalabilidade e performance superior.

---

## ‚úÖ Diferenciais Implementados

* **üöÄ Carga Otimizada (Bulk Insert):**
* A estrat√©gia de `LOAD DATA` √© aproximadamente **20x mais r√°pida** que inser√ß√µes linha a linha (`INSERT INTO`), viabilizando a reimporta√ß√£o frequente do banco.


* **üõ°Ô∏è Tratamento de Casos de Borda (Crescimento):**
* Na an√°lise de crescimento percentual (Query 1), o script utiliza `INNER JOIN` entre trimestres extremos para excluir matematicamente operadoras que n√£o existiam no in√≠cio ou fim do per√≠odo, evitando divis√µes por zero ou resultados tendenciosos.

---

### ‚ñ∂Ô∏è Como Executar

#### Passo a Passo

1.  **Acesse o diret√≥rio dos scripts:**
    ```bash
    cd 03_analise_sql/sql
    ```


2.  **Crie a Estrutura (DDL):**
    No seu cliente SQL (IntelliJ/Workbench), execute o arquivo:
    `01_ddl_create_tables.sql`



3.  **Execute a Carga (Import):**
    Primeiro, habilite a importa√ß√£o local no console do banco:
    ```sql
    SET GLOBAL local_infile = 1;
    ```
    Em seguida, ajuste o caminho do CSV no script e execute:
    `02_import_data.sql`


4.  **Gere os Relat√≥rios:**
    Execute o script de an√°lise:
    `03_queries_analiticas.sql`

**Resultado:** As tabelas do banco `ans_analytics` ser√£o populadas e as queries retornar√£o os rankings de crescimento, distribui√ß√£o por UF e consist√™ncia financeira.

---

## üåê M√≥dulo 4: Plataforma Web & API Analytics

**Localiza√ß√£o:** [`./04_plataforma_web`](https://www.google.com/search?q=./04_plataforma_web)

**Tecnologia:** Python 3.10+ (FastAPI), Vue.js 3, Chart.js, Bootstrap 5.

Este m√≥dulo √© a camada de apresenta√ß√£o e distribui√ß√£o da solu√ß√£o. Ele consiste em uma API REST de alta performance que exp√µe os dados do *Data Warehouse* e um Dashboard Interativo *Single Page Application* (SPA) para visualiza√ß√£o de m√©tricas financeiras em tempo real.

### üõ†Ô∏è Arquitetura da Solu√ß√£o

O sistema opera em uma arquitetura desacoplada (Client-Server):

1. **Backend Ass√≠ncrono (`main.py`):**
* Desenvolvido com **FastAPI**, utilizando `Uvicorn` como servidor ASGI.
* Gerencia conex√µes com o MySQL via *Connection Pooling* para suportar m√∫ltiplas requisi√ß√µes simult√¢neas.
* Exp√µe endpoints RESTful (`/operadoras`, `/estatisticas`) com documenta√ß√£o autom√°tica (Swagger UI).


2. **Frontend Reativo (`index.html`):**
* Constru√≠do com **Vue.js 3** (via CDN) para reatividade sem necessidade de *build steps* complexos.
* Consome a API via `fetch` ass√≠ncrono.
* Renderiza gr√°ficos vetoriais com **Chart.js** e tabelas paginadas com estiliza√ß√£o **Bootstrap**.



---

## üß† Decis√µes T√©cnicas e Trade-offs (An√°lise Cr√≠tica)

Conforme os crit√©rios de avalia√ß√£o, abaixo est√£o as justificativas para as decis√µes de engenharia de software adotadas:

### 1. Framework Backend: FastAPI vs Flask/Django

* **Decis√£o:** FastAPI.
* **Contexto:** Necessidade de servir dados agregados de milh√µes de linhas com baixa lat√™ncia.
* **Justificativa:** Diferente do Flask (s√≠ncrono por padr√£o) ou Django (monol√≠tico e pesado), o FastAPI oferece suporte nativo a concorr√™ncia (`async/await`). Isso permite que o servidor processe novas requisi√ß√µes enquanto aguarda o I/O do banco de dados, maximizando o *throughput* sem bloquear a thread principal.

### 2. Arquitetura Frontend: *No-Build* (CDN) vs *Bundler* (Vite/Webpack)

* **Decis√£o:** Abordagem *No-Build* (Importa√ß√£o via CDN).
* **Contexto:** O projeto precisa ser avaliado rapidamente por recrutadores/revisores.
* **Justificativa:** Elimina a necessidade de instalar Node.js, gerenciar `node_modules` ou rodar pipelines de *build*. O avaliador precisa apenas abrir o arquivo `index.html` no navegador. Reduz a complexidade acidental e a barreira de entrada para execu√ß√£o.

### 3. Gest√£o de Concorr√™ncia (Race Conditions)

* **Problema:** Ao carregar o dashboard, as requisi√ß√µes de "Totais Financeiros" e "Lista de Operadoras" eram disparadas simultaneamente. O controlador de requisi√ß√µes original cancelava a primeira para priorizar a √∫ltima (*AbortController* agressivo), resultando em dados zerados.
* **Solu√ß√£o:** Refatora√ß√£o da camada de servi√ßo HTTP (`safeFetch`). Implementou-se um controle granular onde apenas buscas repetitivas (digita√ß√£o no campo de pesquisa) s√£o canceladas (Debounce), enquanto cargas iniciais cr√≠ticas correm em paralelo (*Parallel Fetching*), garantindo a exibi√ß√£o correta dos **R$ 47 Trilh√µes**.

---

## ‚úÖ Diferenciais Implementados

* **‚ö° Otimiza√ß√£o de UX (Debounce):**
* Implementa√ß√£o de l√≥gica de *Debounce* na barra de busca. A requisi√ß√£o ao backend s√≥ √© disparada ap√≥s o usu√°rio parar de digitar por 450ms, prevenindo inunda√ß√µes de requisi√ß√µes (*Flood*) no banco de dados.


* **üìä Visualiza√ß√£o de Big Data:**
* O dashboard foi calibrado para renderizar e formatar corretamente volumes financeiros na casa dos **Trilh√µes**, tratando precis√£o de ponto flutuante e localiza√ß√£o (PT-BR) no Frontend.


* **üõ°Ô∏è Seguran√ßa e CORS:**
* Configura√ß√£o expl√≠cita de *Cross-Origin Resource Sharing* (CORS) no Backend para permitir comunica√ß√£o segura entre diferentes origens locais (`localhost` vs `127.0.0.1`), um erro comum em ambientes de desenvolvimento Windows.



---

## ‚ñ∂Ô∏è Como Executar

### Pr√©-requisitos

* Python 3.10+
* Pip (Gerenciador de pacotes Python)
* Navegador Moderno (Chrome/Edge/Firefox)

### Passo a Passo

1. **Acesse o diret√≥rio do m√≥dulo:**
```bash
cd 04_plataforma_web
```


2. **Instale as depend√™ncias:**
```bash
pip install fastapi uvicorn mysql-connector-python
```


3. **Inicie o Servidor Backend:**
```bash
python -m uvicorn main:app --reload
```


*O terminal exibir√°: `Uvicorn running on http://127.0.0.1:8000*`
4. **Acesse o Dashboard:**
* V√° at√© a pasta `04_plataforma_web` pelo seu explorador de arquivos.
* D√™ um duplo clique no arquivo **`index.html`**.
* *(Opcional)* Para testar a API pura, acesse: `http://127.0.0.1:8000/docs`



**Resultado:** O painel carregar√° os KPIs financeiros e o gr√°fico de distribui√ß√£o por UF instantaneamente.